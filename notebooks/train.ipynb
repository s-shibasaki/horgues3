{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, Any, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "from horgues3.dataset import HorguesDataset\n",
    "from horgues3.models import HorguesModel, WeightedPlackettLuceLoss\n",
    "\n",
    "\n",
    "# ログ設定\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # データ設定\n",
    "    'train_start_date': '20231201',\n",
    "    'train_end_date': '20231231',\n",
    "    'val_start_date': '20240101',\n",
    "    'val_end_date': '20240131',\n",
    "    'num_horses': 18,\n",
    "    'horse_history_length': 18,\n",
    "    'history_days': 365,\n",
    "    'exclude_hours_before_race': 2,\n",
    "    'cache_dir': 'cache',\n",
    "    'use_cache': True,\n",
    "    \n",
    "    # モデル設定\n",
    "    'd_token': 768,\n",
    "    'num_bins': 8,\n",
    "    'binning_temperature': 0.8,\n",
    "    'binning_init_range': 2.5,\n",
    "    'ft_n_layers': 2,\n",
    "    'ft_n_heads': 12,\n",
    "    'ft_d_ffn': 1536,\n",
    "    'seq_n_layers': 3,\n",
    "    'seq_n_heads': 12,\n",
    "    'seq_d_ffn': 2304,\n",
    "    'race_n_layers': 4,\n",
    "    'race_n_heads': 12,\n",
    "    'race_d_ffn': 3072,\n",
    "    'dropout': 0.5,\n",
    "    \n",
    "    # 損失関数設定\n",
    "    'loss_temperature': 1.0,\n",
    "    'loss_top_k': None,\n",
    "    'weight_decay': 0.8,\n",
    "    \n",
    "    # 学習設定\n",
    "    'batch_size': 8,\n",
    "    'num_epochs': 100,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay_optimizer': 1e-2,\n",
    "    'warmup_epochs': 10,\n",
    "    'early_stopping_patience': 15,\n",
    "    'save_every_n_epochs': 10,\n",
    "    \n",
    "    # システム設定\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"1エポックの学習\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # データをデバイスに移動\n",
    "        x_num = {k: v.to(device) for k, v in batch['x_num'].items()}\n",
    "        x_cat = {k: v.to(device) for k, v in batch['x_cat'].items()}\n",
    "        sequence_data = {}\n",
    "        for seq_name, seq_data in batch['sequence_data'].items():\n",
    "            sequence_data[seq_name] = {\n",
    "                'x_num': {k: v.to(device) for k, v in seq_data['x_num'].items()},\n",
    "                'x_cat': {k: v.to(device) for k, v in seq_data['x_cat'].items()},\n",
    "                'mask': seq_data['mask'].to(device)\n",
    "            }\n",
    "        mask = batch['mask'].to(device)\n",
    "        rankings = batch['rankings'].to(device)\n",
    "        \n",
    "        # 前向き計算\n",
    "        scores = model(x_num, x_cat, sequence_data, mask)\n",
    "        loss = criterion(scores, rankings, mask)\n",
    "        \n",
    "        # 後向き計算\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"1エポックの検証\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # データをデバイスに移動\n",
    "            x_num = {k: v.to(device) for k, v in batch['x_num'].items()}\n",
    "            x_cat = {k: v.to(device) for k, v in batch['x_cat'].items()}\n",
    "            sequence_data = {}\n",
    "            for seq_name, seq_data in batch['sequence_data'].items():\n",
    "                sequence_data[seq_name] = {\n",
    "                    'x_num': {k: v.to(device) for k, v in seq_data['x_num'].items()},\n",
    "                    'x_cat': {k: v.to(device) for k, v in seq_data['x_cat'].items()},\n",
    "                    'mask': seq_data['mask'].to(device)\n",
    "                }\n",
    "            mask = batch['mask'].to(device)\n",
    "            rankings = batch['rankings'].to(device)\n",
    "            \n",
    "            # 前向き計算\n",
    "            scores = model(x_num, x_cat, sequence_data, mask)\n",
    "            loss = criterion(scores, rankings, mask)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(f\"outputs/training_{timestamp}\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Output directory: {output_dir}\")\n",
    "logger.info(f\"Device: {config['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Preparing training dataset...\")\n",
    "train_dataset = HorguesDataset(\n",
    "    start_date=config['train_start_date'],\n",
    "    end_date=config['train_end_date'],\n",
    "    num_horses=config['num_horses'],\n",
    "    horse_history_length=config['horse_history_length'],\n",
    "    history_days=config['history_days'],\n",
    "    exclude_hours_before_race=config['exclude_hours_before_race'],\n",
    "    cache_dir=config['cache_dir'],\n",
    "    use_cache=config['use_cache']\n",
    ")\n",
    "\n",
    "preprocessing_params = train_dataset.get_preprocessing_params()\n",
    "\n",
    "logger.info(\"Preparing validation dataset...\")\n",
    "val_dataset = HorguesDataset(\n",
    "    start_date=config['val_start_date'],\n",
    "    end_date=config['val_end_date'],\n",
    "    num_horses=config['num_horses'],\n",
    "    horse_history_length=config['horse_history_length'],\n",
    "    history_days=config['history_days'],\n",
    "    exclude_hours_before_race=config['exclude_hours_before_race'],\n",
    "    preprocessing_params=preprocessing_params,\n",
    "    cache_dir=config['cache_dir'],\n",
    "    use_cache=config['use_cache']\n",
    ")\n",
    "\n",
    "logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "logger.info(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# データローダー準備\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [feature_name for feature_name in preprocessing_params['scaler'].keys()]\n",
    "categorical_features = {feature_name: max(encoder_param.values()) for feature_name, encoder_param in preprocessing_params['encoder'].items()}\n",
    "\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "sequence_names = list(sample_batch['sequence_data'].keys())\n",
    "feature_aliases = train_dataset.get_feature_aliases()\n",
    "\n",
    "logger.info(f\"Numerical features: {numerical_features}\")\n",
    "logger.info(f\"Categorical features: {categorical_features}\")\n",
    "logger.info(f\"Sequence names: {sequence_names}\")\n",
    "logger.info(f\"Feature aliases: {feature_aliases}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HorguesModel(\n",
    "    sequence_names=sequence_names,\n",
    "    feature_aliases=feature_aliases,\n",
    "    numerical_features=numerical_features,\n",
    "    categorical_features=categorical_features,\n",
    "    d_token=config['d_token'],\n",
    "    num_bins=config['num_bins'],\n",
    "    binning_temperature=config['binning_temperature'],\n",
    "    binning_init_range=config['binning_init_range'],\n",
    "    ft_n_layers=config['ft_n_layers'],\n",
    "    ft_n_heads=config['ft_n_heads'],\n",
    "    ft_d_ffn=config['ft_d_ffn'],\n",
    "    seq_n_layers=config['seq_n_layers'],\n",
    "    seq_n_heads=config['seq_n_heads'],\n",
    "    seq_d_ffn=config['seq_d_ffn'],\n",
    "    race_n_layers=config['race_n_layers'],\n",
    "    race_n_heads=config['race_n_heads'],\n",
    "    race_d_ffn=config['race_d_ffn'],\n",
    "    dropout=config['dropout']\n",
    ").to(config['device'])\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Total parameters: {total_params:,}\")\n",
    "logger.info(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "criterion = WeightedPlackettLuceLoss(\n",
    "    temperature=config['loss_temperature'],\n",
    "    top_k=config['loss_top_k'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    reduction='mean'\n",
    ")\n",
    "\n",
    "# オプティマイザ\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay_optimizer']\n",
    ")\n",
    "\n",
    "# スケジューラ\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config['num_epochs'] - config['warmup_epochs'],\n",
    "    eta_min=config['learning_rate'] * 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'epochs': [],\n",
    "    'learning_rates': [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    logger.info(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "\n",
    "    # ウォームアップ期間中は学習率を線形に増加\n",
    "    if epoch < config['warmup_epochs']:\n",
    "        lr = config['learning_rate'] * (epoch + 1) / config['warmup_epochs']\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    logger.info(f\"Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, config['device'])\n",
    "    val_loss = validate_epoch(model, val_dataloader, criterion, config['device'])\n",
    "    logger.info(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['epochs'].append(epoch + 1)\n",
    "    history['learning_rates'].append(current_lr)\n",
    "\n",
    "    # ベストモデル保存\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config,\n",
    "            'preprocessing_params': preprocessing_params,\n",
    "            'feature_info': {\n",
    "                'numerical_features': numerical_features,\n",
    "                'categorical_features': categorical_features,\n",
    "                'sequence_names': sequence_names,\n",
    "                'feature_aliases': feature_aliases\n",
    "            }\n",
    "        }, output_dir / 'best_model.pth')\n",
    "\n",
    "        logger.info(f\"New best model saved (val_loss: {val_loss:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # 定期保存\n",
    "    if (epoch + 1) % config['save_every_n_epochs'] == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'config': config,\n",
    "            'preprocessing_params': preprocessing_params,\n",
    "            'feature_info': {\n",
    "                'numerical_features': numerical_features,\n",
    "                'categorical_features': categorical_features,\n",
    "                'sequence_names': sequence_names,\n",
    "                'feature_aliases': feature_aliases\n",
    "            }\n",
    "        }, output_dir / f'checkpoint_epoch_{epoch+1}.pth')\n",
    "        \n",
    "        logger.info(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "    \n",
    "    # 学習履歴保存\n",
    "    with open(output_dir / 'history.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(history, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # 早期停止\n",
    "    if patience_counter >= config['early_stopping_patience']:\n",
    "        logger.info(f\"Early stopping triggered after {epoch+1} epochs (patience: {config['early_stopping_patience']})\")\n",
    "        break\n",
    "\n",
    "# 最終モデル保存\n",
    "torch.save({\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None,\n",
    "    'train_loss': train_loss,\n",
    "    'val_loss': val_loss,\n",
    "    'config': config,\n",
    "    'preprocessing_params': train_dataset.get_preprocessing_params(),\n",
    "    'feature_info': {\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'sequence_names': sequence_names,\n",
    "        'feature_aliases': feature_aliases\n",
    "    }\n",
    "}, output_dir / 'final_model.pth')\n",
    "\n",
    "logger.info(\"Training completed!\")\n",
    "logger.info(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "logger.info(f\"Final train loss: {train_loss:.4f}\")\n",
    "logger.info(f\"Final validation loss: {val_loss:.4f}\")\n",
    "logger.info(f\"Results saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "horgues3",
   "language": "python",
   "name": "horgues3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
